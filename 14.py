import requests
from bs4 import BeautifulSoup
import logging # ì›¹í˜ì´ì§€ì—ì„œ ì§€ì •ëœ ê°’ ì´ì™¸ì— ë³€ìˆ˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ëª©ì 

""" @@@@@ ì½”ë“œì±Œë¦°ì§€_04 @@@@@
berlinstartupjobs.com ì›¹ì‚¬ì´íŠ¸ìš© ìŠ¤í¬ë˜í¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.
ìŠ¤í¬ë˜í¼ëŠ” ë‹¤ìŒ URLì„ ìŠ¤í¬ë©í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
https://berlinstartupjobs.com/engineering/
https://berlinstartupjobs.com/skill-areas/python/
https://berlinstartupjobs.com/skill-areas/typescript/
https://berlinstartupjobs.com/skill-areas/javascript/
ì²« ë²ˆì§¸ URLì—ëŠ” í˜ì´ì§€ê°€ ìˆìœ¼ë¯€ë¡œ pagination ì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
ë‚˜ë¨¸ì§€ URLì€ íŠ¹ì • ìŠ¤í‚¬ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤. URLì˜ êµ¬ì¡°ì— ìŠ¤í‚¬ ì´ë¦„ì´ ìˆìœ¼ë¯€ë¡œ ëª¨ë“  ìŠ¤í‚¬ì„ ìŠ¤í¬ë˜í•‘í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë˜í¼ë¥¼ ë§Œë“œì„¸ìš”.
íšŒì‚¬ ì´ë¦„, ì§ë¬´ ì œëª©, ì„¤ëª… ë° ì§ë¬´ ë§í¬ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. 

"""


class Scrape:
    all_jobs= []
    def __init__(self,url):

        response = requests.get(url)
        soup = BeautifulSoup(
            response.content,
            "html.parser"
        )
        self.url = soup
        
# ì§ë¬´ì œëª©,íšŒì‚¬ì´ë¦„, ì§ë¬´ë§í¬, ì„¤ëª…
# bjs-jild__h https://berlinstartupjobs.com/engineering/senior-golang-engineer-f-m-d-berlin-hybrid-fincompare/
# bjs-jlid__b https://berlinstartupjobs.com/companies/fincompare/
# links-box    bjs-bl bjs-bl-whisper # êµ¬ì§ëª…
# bjs-jlid__description

    def scrape_page(self,soup,all_jobs): 

        jobs = soup.find("li",
                class_ ="bjs-jild"
            ).find_all("li") # [:-1]
        for job in jobs:
            title = job.find("h4",class_="bjs-jild__h").text
            company= job.find("a", class_="bjs-jlid__b").text   
            description = job.find("div", class_="bjs-jlid__description").text
            url = job.find_all("div", class_="links-box").next_sibling["a"]
            job_data ={     
                "title": title,
                "company": company.text,
                "description": description.text,
                "url": f"https://berlinstartupjobs.com/skill-areas/{url}",
            }
            all_jobs.append(job_data)
class Pagination:
          
    def get_pages(url):    
        response =requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
        #class ëŠ” class ë©”ì„œë“œì™€ì˜ êµ¬ë³„ì„ ìœ„í—¤ '_'ë¥¼ ë¶™ì—¬ì„œ ì‚¬ìš©í•œë‹¤. ê·¸ ì™¸ ê¸°ëŠ¥ì€ ì—†ë‹¤. '_'ë¥¼ ë¶™ì´ì§€ ì•Šìœ¼ë©´ êµ¬ë¬¸ì— ì˜¤ë¥˜ê°€ ë‚œë‹¤.
        buttons =  len(soup.find("ul", class_="bsj-now").find_all("a", class_="page-numbers")) #page ì–‘ì„ í†µí•´ ë°˜ë³µë¬¸ íšŸìˆ˜ë¥¼ ê²°ì •í•œë‹¤.
        return buttons
    totla_pages = get_pages("https://berlinstartupjobs.com/engineering/page/1/")
    #scrap_pageì—ì„œ ë”°ë¡œ url ì„ ì •ì˜í•˜ì§€ ì•ŠëŠ” ê±´ í•˜ê¸° ë°˜ë³µë¬¸ìœ¼ë¡œ ì „ì²´ í˜ì´ì§• ì½”ë“œë¥¼ ì‘ì„±í•˜ê¸° ë•Œë¬¸.
    for x in range(totla_pages):
        # ë¦¬ìŠ¤íŠ¸ì— íŠ¹ì„±ì— ë”°ë¼ í˜ì´ì§€ 1ë¶€í„°ë¡œ ì„ì˜ë¡œ ìˆ«ìë¥¼ ë³€ê²½í•¨.
        url = f"https://berlinstartupjobs.com/engineering/page/{x+1}/"
        #scrape_page(url)

    # print(len(all_jobs))
    # print(all_jobs)

    #í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ  "remotelOk" ì‚¬ì´íŠ¸ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì¼ìë¦¬ ê²€ìƒ‰í•˜ê¸°

    keywords = [
        "python",
        "typescript",
        "javascript",
    ]
    #ë¡œì»¬ì—ì„œ ì›¹ì„œë²„ì— ì ‘ì†ì‹œë„ì‹œ 
    r = requests.get("https://berlinstartupjobs.com/skill-areas/{keywords}",
    )
    # print(r.status_code)# 503
    # print(r.content)

    # ì›¹ì—ì„œ ì‚¬ì´íŠ¸ì— ì ‘ì†ì‹œë„í•˜ëŠ” ê²ƒì²˜ëŸ¼ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ headerì •ë³´ë¡œ ì ‘ì†ì‹œë„ì‹œ
    r = requests.get("https://berlinstartupjobs.com/skill-areas/{keywords}", headers={
        "User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    }
    )
    # https://remoteok.com/remote-{í‚¤ì›Œë“œ}-jobs 
    # def í•¨ìˆ˜ë¡œ ë§Œë“¤ë˜, classë¡œ ë³€í˜•ë„ ì‹œì¼œë³´ì: ëª¨ë“  ì¼ìë¦¬ ì •ë³´ë¥¼ class ì— ë„£ê³  methodë¥¼ í™œìš©í•´ì„œ jobë“¤ì„ ë°°ì—´ì•ˆì— ë„£ì–´ë³´ì.
    # OOPí˜•íƒœë¡œì˜ ë³€í™˜ë„ ì‹œë„í•´ë´ë¼. ğŸ’©

    print(r.content)